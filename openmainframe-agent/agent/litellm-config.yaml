# LiteLLM Proxy Configuration for OpenMainframe Agent
# Start with: litellm --config litellm-config.yaml
# Docs: https://docs.litellm.ai/docs/tutorials/claude_agent_sdk

model_list:
  # --- Anthropic (direct API) ---
  - model_name: claude-sonnet
    litellm_params:
      model: anthropic/claude-sonnet-4-5-20250929
      # Uses ANTHROPIC_API_KEY env var

  - model_name: claude-opus
    litellm_params:
      model: anthropic/claude-opus-4-20250514
      # Uses ANTHROPIC_API_KEY env var

  # --- AWS Bedrock (uncomment and set AWS credentials) ---
  # - model_name: bedrock-claude-sonnet
  #   litellm_params:
  #     model: bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0
  #     aws_region_name: us-east-1

  # --- Azure OpenAI (uncomment and configure) ---
  # - model_name: azure-gpt4
  #   litellm_params:
  #     model: azure/gpt-4
  #     api_base: https://your-resource.openai.azure.com/
  #     api_key: os.environ/AZURE_API_KEY
  #     api_version: "2024-02-15-preview"

  # --- OpenAI (uncomment if needed) ---
  # - model_name: gpt-4.1
  #   litellm_params:
  #     model: openai/gpt-4.1
  #     # Uses OPENAI_API_KEY env var

litellm_settings:
  drop_params: true  # Drop unsupported params instead of erroring
